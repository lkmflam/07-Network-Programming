# Consuming web services in Python with urllib
In this section, we will learn how to use urllib and how we can build HTTP clients with this module.

The urllib module allows access to any resource published on the network (web page, files, directories, images, and so on) through various protocols (HTTP, FTP, SFTP). To start consuming a web service, we have to import the following libraries:
```python
#! /usr/bin/env python3
import urllib.request
import urllib.parse
```
There are four functions in urllib:

request: Opens and reads the request's URL
error: Contains the errors generated by the request
parse: A tool to convert the URL
robotparse: Converts the robots.txt files
The urllib.request module allows access to a resource published on the internet through its address. If we go to the documentation of the Python 3 module (https://docs.python.org/3/library/urllib.request.html#module-urllib.request), we will see all the functions that have this class. The main one is urlopen, which works in the following way.

A urlopen function is used to create an object similar to a file, with which to read from the URL. This object has methods such as read, readline, readlines, and close, which work exactly the same as in the file objects, although in reality, we are working with wrapper's methods that abstract us from using sockets at a low level.

The urlopen function has an optional data parameter with which to send information to HTTP addresses using the POST method, where parameters are sent in the request itself; for example, to respond to a form. This parameter is a properly encoded string:
```python
urllib.request.urlopen (url, data = None, [timeout,] *, cafile = None, capath = None, cadefault = False, context = None)
```
Retrieving the contents of a URL is a straightforward process when done using urllib. You can open the Python interpreter and execute the following instructions:
```python
>>> from urllib.request import urlopen
>>> response = urlopen('http://www.example.com')
>>> response
<http.client.HTTPResponse object at 0x7fa3c53059b0>
>>> response.readline()
```
We use the urllib.request.urlopen() function to send a request and receive a response for the resource at http://www.example.com, in this case an HTML page. We will then print out the first line of the HTML we receive, with the readline() method from the response object.

This function also supports specifying a timeout for the request that represents the waiting time in the request; that is, if the page takes more than what we indicated, it will result in an error:
```python
>>> print(urllib.request.urlopen(“http://example.com”,timeout=30))
```
We can see from the preceding example that urlopen() returns an http.client.HTTPResponse instance. The response object gives us access to the data of the requested resource and the properties and the metadata of the response:
```
<http.client.HTTPResponse object at 0x03C4DC90>
```
If we get a response in JSON format, we can use the following Python json module:
```python
>>> import json
>>> response = urllib.request.urlopen(url,timeout=30)
>>> json_response = json.loads(response.read())
```
In the variable response, we save the file that launches the request, and we use the read() function to read the content. Then we transform it into JSON format.
## Status codes
HTTP responses provide us with a way to check the status of the response through status codes. We can read the status code of a response using its status property. The value of 200 is an HTTP status code that tells us that the request is OK:
```
>>> response.status
200
```
The 200 code informs us that everything went fine. There are a number of codes, and each one conveys a different meaning. According to their first digit, status codes are classified into the following groups:
```
100: Informational
200: Success
300: Redirection
400: Client error
500: Server error
```
Status codes help us to see whether our response was successful or not. Any code in the 200 range indicates a success, whereas any code in either the 400 range or the 500 range indicates failure in the server.

The official list of status codes is maintained by IANA and can be found at https://www.iana.org/assignments/http-status-codes.

## Handling exceptions
Status codes should always be checked so that our program can respond appropriately if something goes wrong. The urllib package helps us in checking the status codes by raising an exception if it encounters a problem.

Let's go through how to catch these and handle them usefully. We'll try this following command block. You can find the following code in the urllib_exceptions.py file:
```python
import urllib.error
from urllib.request import urlopen
try:
   urlopen('http://www.ietf.org/rfc/rfc0.txt')
except urllib.error.HTTPError as e:
    print('Exception', e)
    print('status', e.code)
    print('reason', e.reason)
    print('url', e.url)
```
The output of the previous script is:
```
Exception HTTP Error 404: Not Found
status 404
reason Not Found
url https://www.ietf.org/rfc/rfc0.txt
```
In the previous script, we've requested an rfc0.txt document, which doesn't exist. So the server has returned a 404 status code, and urllib has captured this and raised an HTTPError. You can see that HTTPError provides useful attributes regarding the request. In the preceding example, we obtain the status, reason, and url attributes to get some information about the response.

## HTTP headers
A request to the server consists of a request line that contains some basic information about the request, and various lines that constitute the headers.

HTTP requests consist of two main parts: a header and a body. Headers are the lines of information that contain specific metadata about the response and tell the client how to interpret it. With this module, we can check whether the headers can provide information about the web server.

The HTTP headers are Name: value pairs; for example, Host: www.example.com. These headers contain different information about the HTTP request and about the browser. For example, the User-Agent line provides information about the browser and operating system of the machine from which the request is made, and Accept Encoding informs the server if the browser can accept compressed data under formats such as gzip.

An important header is the host header. Many web server applications provide the ability to host more than one website on the same server using the same IP address. DNS aliases are set up for the various website domain names, so they all point to the same IP address. Effectively, the web server is given multiple hostnames, one for each website it hosts.

The following script will obtain the site headers through the response object's headers. For this task, we can use the headers property or the getheaders() method. The getheaders() method returns the headers as a list of tuples of the form (header name, header value).

You can find the following code in the get_headers.py file: 
```python
#!/usr/bin/env python3
import urllib.request
url = input("Enter the URL:")
http_response = urllib.request.urlopen(url)
if http_response.code == 200:
    print(http_response.headers)
    for key,value in http_response.getheaders():
        print(key,value)
```
Output:
```
Enter the URL:http://www.example.com
Server: nginx/1.4.5
Date: Mon, 26 Nov 2019 11:04:05 GMT
Content-Type: text/html; charset=utf-8
Transfer-Encoding: chunked
Connection: close
```
